{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total points for this HW: 100.\n",
    "\n",
    "Please note: Copying and pasting other people's work is absolutely prohibited.  Any such cases will be reported to CUSP's education team and severely punished. Discussion is encouraged, and feel free to exchange ideas with your classmates, but please write your own code and do your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 1: Accuracy and interpretability (10 pts)\n",
    "\n",
    "a) Describe a real-world prediction problem using urban data for which _interpretability_ of your models and results is essential, and for which it might be preferable to use decision trees rather than random forests.  Argue why this is the case. (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A model based urban planning policy is where interpretability is critical. Because this is an exercise that involves **convincing multiple stakeholders to buy-in**. Being able to explain the reasons, which might be based on a predictive model output, is essential. In this case, simple, but highly interpretable model, such as a single decision tree, is preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe a real-world prediction problem using urban data for which _accuracy_ is paramount and interpretability may be less important, and for which it might be preferable to use random forests rather than decision trees.  Argue why this is the case. (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A predictive model based Stop-and-Frisk solution, using demographic and crime history data, will be a great example of where accuracy is essential. **Productivity is crucial in large scale operations**, hence having high accuracy can allow police to save time by avoiding wasting time on the wrong suspects. \n",
    "\n",
    "In this case, a random forest with solid accuracy performance will be preferrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Let's imagine that you want to try to get the best of both worlds (accuracy _and_ interpretability).  So you decide to start by learning a random forest classifier.  Describe at least one way of getting some interpretability out of the model by post-processing.  You could either pick a method from the literature (e.g., Domingos's work on combining multiple models or some method of computing variable importance), or come up with your own approach (doesn't have to be ground-breaking, but feel free to be creative!) (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: I will try to use the out-of-box feature importance output from Random Forest. Although it is not as straightforward as looking at the tree split from a decision tree, it provides a good sense of the power of the features on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2: Build a decision tree for classification, step by step, following the lecture notes. Note that the dataset has been slightly modified, so you will get a different tree than the one shown in the lecture notes.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>HP</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>175</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>139</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>190</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>170</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MPG  cylinders   HP   weight\n",
       "0   good          4   75    light\n",
       "1    bad          6   90   medium\n",
       "2    bad          4  110   medium\n",
       "3    bad          8  175  weighty\n",
       "4    bad          6   95   medium\n",
       "5    bad          4   94    light\n",
       "6    bad          4   95    light\n",
       "7    bad          8  139  weighty\n",
       "8    bad          8  190  weighty\n",
       "9    bad          8  145  weighty\n",
       "10   bad          6  100   medium\n",
       "11  good          4   92   medium\n",
       "12   bad          6  100  weighty\n",
       "13   bad          8  170  weighty\n",
       "14  good          4   89   medium\n",
       "15  good          4   65    light\n",
       "16   bad          6   85   medium\n",
       "17  good          4   81    light\n",
       "18   bad          6   95   medium\n",
       "19   bad          4   93    light"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StringIO library is python version dependent\n",
    "try:\n",
    "    # python 3\n",
    "    from io import StringIO\n",
    "except: \n",
    "    # python 2\n",
    "    from StringIO import StringIO\n",
    "\n",
    "thefile = StringIO('MPG,cylinders,HP,weight\\ngood,4,75,light\\nbad,6,90,medium\\nbad,4,110,medium\\nbad,8,175,weighty\\nbad,6,95,medium\\nbad,4,94,light\\nbad,4,95,light\\nbad,8,139,weighty\\nbad,8,190,weighty\\nbad,8,145,weighty\\nbad,6,100,medium\\ngood,4,92,medium\\nbad,6,100,weighty\\nbad,8,170,weighty\\ngood,4,89,medium\\ngood,4,65,light\\nbad,6,85,medium\\ngood,4,81,light\\nbad,6,95,medium\\nbad,4,93,light')\n",
    "df = pd.read_csv(thefile)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please use numpy and pandas to do the computation for parts a) through f).  Do not use an existing decision tree implementation like sklearn for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Start with the entire dataset and find the most common MPG value. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG            object\n",
       "cylinders    category\n",
       "HP              int64\n",
       "weight       category\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Column Type\n",
    "df['cylinders'] = df['cylinders'].astype('category')\n",
    "df['weight'] = df['weight'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most common MPG value is 'bad'; count is 15\n"
     ]
    }
   ],
   "source": [
    "def findMPG(target):\n",
    "    \n",
    "    label, count = np.unique(target, return_counts=True)\n",
    "    print(\"the most common MPG value is '{}'; count is {}\".format(label[np.argmax(count)], count.max()))\n",
    "    \n",
    "    return\n",
    "\n",
    "findMPG(df['MPG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InformationGain(goodY,badY,goodN,badN):\n",
    "    def F(X,Y):\n",
    "        val1 = X*np.log2(1.*(X+Y)/X) if X>0 else 0\n",
    "        val2 = Y*np.log2(1.*(X+Y)/Y) if Y>0 else 0\n",
    "        return val1+val2\n",
    "    return (F(goodY+goodN,badY+badN)-F(goodY,badY)-F(goodN,badN)) / (goodY+goodN+badY+badN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Enumerate all the possible binary questions you could ask for each discrete-valued variable.  For each such split, compute the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, and compute the information gain using the provided function above. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Splitting on cylinders = 4\n",
      "# of Good in Y: 5\n",
      "# of Bad in Y: 4\n",
      "# of Good in N: 0\n",
      "# of Bad in N: 11\n",
      "Info Gain: 0.3652938975319328\n",
      "---\n",
      "Splitting on cylinders = 6\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 6\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 9\n",
      "Info Gain: 0.15307795338969116\n",
      "---\n",
      "Splitting on cylinders = 8\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 5\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 10\n",
      "Info Gain: 0.1225562489182657\n",
      "---\n",
      "Splitting on weight = light\n",
      "# of Good in Y: 3\n",
      "# of Bad in Y: 3\n",
      "# of Good in N: 2\n",
      "# of Bad in N: 12\n",
      "Info Gain: 0.09710717945150363\n",
      "---\n",
      "Splitting on weight = medium\n",
      "# of Good in Y: 2\n",
      "# of Bad in Y: 6\n",
      "# of Good in N: 3\n",
      "# of Bad in N: 9\n",
      "Info Gain: 0.0\n",
      "---\n",
      "Splitting on weight = weighty\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 6\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 9\n",
      "Info Gain: 0.15307795338969116\n"
     ]
    }
   ],
   "source": [
    "def CategoricalRules(df):\n",
    "    \n",
    "    # Find Categorical Columns (exclude MPG)\n",
    "    cols = list(df.iloc[:, 1:len(df.columns)].select_dtypes(include = ['category', 'object']))\n",
    "    \n",
    "    for col in cols: \n",
    "        \n",
    "        # Find Unique Values\n",
    "        unique_val = np.unique(df[col])     \n",
    "        \n",
    "        # Split Data by Values\n",
    "        \n",
    "        for rule in unique_val:\n",
    "            \n",
    "            # Seperate full data based on the split condition\n",
    "            Y = df[df[col] == rule].iloc[:, 0]\n",
    "            N = df[df[col] != rule].iloc[:, 0]\n",
    "            \n",
    "            # Calculate the number of Good / Bad in each split\n",
    "            goodY = np.count_nonzero(Y == 'good')\n",
    "            badY = np.count_nonzero(Y == 'bad')\n",
    "            goodN = np.count_nonzero(N == 'good')\n",
    "            badN = np.count_nonzero(N == 'bad')\n",
    "            \n",
    "            # Calculate Infomation Gain for the Split\n",
    "            infoGain = InformationGain(goodY,badY,goodN,badN)\n",
    "            \n",
    "            print('---')\n",
    "            print('Splitting on {} = {}'.format(col, rule))\n",
    "            print('# of Good in Y: {}'.format(goodY))\n",
    "            print('# of Bad in Y: {}'.format(badY))\n",
    "            print('# of Good in N: {}'.format(goodN))\n",
    "            print('# of Bad in N: {}'.format(badN))\n",
    "            print(\"Info Gain: {}\".format(infoGain))\n",
    "    \n",
    "    return\n",
    "\n",
    "CategoricalRules(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Enumerate all the possible binary questions you could ask for the real-valued variable HP.  For each such split, compute the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, and compute the information gain using the provided function above. (5 pts) \n",
    "\n",
    "NOTE: if you'd like, you can just use all midpoints between consecutive values of the sorted HP attribute.  You are not required to exclude provably suboptimal questions like we did in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unique Numerical Values ---\n",
      "[ 65  75  81  85  89  90  92  93  94  95 100 110 139 145 170 175 190]\n",
      "--- Mid Points ---\n",
      "[70.0, 78.0, 83.0, 87.0, 89.5, 91.0, 92.5, 93.5, 94.5, 97.5, 105.0, 124.5, 142.0, 157.5, 172.5, 182.5]\n",
      "---\n",
      "Splitting on HP > 70.0\n",
      "# of Good in Y: 4\n",
      "# of Bad in Y: 15\n",
      "# of Good in N: 1\n",
      "# of Bad in N: 0\n",
      "Info Gain: 0.10591493339411553\n",
      "---\n",
      "Splitting on HP > 78.0\n",
      "# of Good in Y: 3\n",
      "# of Bad in Y: 15\n",
      "# of Good in N: 2\n",
      "# of Bad in N: 0\n",
      "Info Gain: 0.22625794497561413\n",
      "---\n",
      "Splitting on HP > 83.0\n",
      "# of Good in Y: 2\n",
      "# of Bad in Y: 15\n",
      "# of Good in N: 3\n",
      "# of Bad in N: 0\n",
      "Info Gain: 0.36710265610273324\n",
      "---\n",
      "Splitting on HP > 87.0\n",
      "# of Good in Y: 2\n",
      "# of Bad in Y: 14\n",
      "# of Good in N: 3\n",
      "# of Bad in N: 1\n",
      "Info Gain: 0.21417094500762923\n",
      "---\n",
      "Splitting on HP > 89.5\n",
      "# of Good in Y: 1\n",
      "# of Bad in Y: 14\n",
      "# of Good in N: 4\n",
      "# of Bad in N: 1\n",
      "Info Gain: 0.36577659947122626\n",
      "---\n",
      "Splitting on HP > 91.0\n",
      "# of Good in Y: 1\n",
      "# of Bad in Y: 13\n",
      "# of Good in N: 4\n",
      "# of Bad in N: 2\n",
      "Info Gain: 0.2759267455941731\n",
      "---\n",
      "Splitting on HP > 92.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 13\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 2\n",
      "Info Gain: 0.5091859254608121\n",
      "---\n",
      "Splitting on HP > 93.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 12\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 3\n",
      "Info Gain: 0.4295045232891469\n",
      "---\n",
      "Splitting on HP > 94.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 11\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 4\n",
      "Info Gain: 0.3652938975319328\n",
      "---\n",
      "Splitting on HP > 97.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 8\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 7\n",
      "Info Gain: 0.22335687046844122\n",
      "---\n",
      "Splitting on HP > 105.0\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 6\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 9\n",
      "Info Gain: 0.15307795338969116\n",
      "---\n",
      "Splitting on HP > 124.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 5\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 10\n",
      "Info Gain: 0.1225562489182657\n",
      "---\n",
      "Splitting on HP > 142.0\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 4\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 11\n",
      "Info Gain: 0.09444753843148686\n",
      "---\n",
      "Splitting on HP > 157.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 3\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 12\n",
      "Info Gain: 0.06839423355087862\n",
      "---\n",
      "Splitting on HP > 172.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 2\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 13\n",
      "Info Gain: 0.044113463674602206\n",
      "---\n",
      "Splitting on HP > 182.5\n",
      "# of Good in Y: 0\n",
      "# of Bad in Y: 1\n",
      "# of Good in N: 5\n",
      "# of Bad in N: 14\n",
      "Info Gain: 0.021377455849890127\n"
     ]
    }
   ],
   "source": [
    "def numercialRules(df):\n",
    "    \n",
    "    # find numberical columns\n",
    "    cols = list(df.iloc[:, 1:len(df.columns)].select_dtypes(exclude = ['category', 'object']))\n",
    "    \n",
    "    # sort and find midpoint of numerical\n",
    "    for col in cols:\n",
    "        unique_val = np.unique(df[cols])\n",
    "        print(\"--- Unique Numerical Values ---\")\n",
    "        print(unique_val)\n",
    "        \n",
    "        # Find midpoints of two sorted values as splitting rules\n",
    "        rules = []\n",
    "        for i in range(0, len(unique_val)):\n",
    "            try:\n",
    "                rules.append((unique_val[i]+unique_val[i+1])/2)\n",
    "            except IndexError:\n",
    "                pass\n",
    "        print(\"--- Mid Points ---\")\n",
    "        print(rules)\n",
    "        \n",
    "        # Split the data based on numerical rules\n",
    "        for rule in rules:\n",
    "            \n",
    "            # Seperate full data based on the split condition\n",
    "            Y = df[df[col] > rule].iloc[:, 0]\n",
    "            N = df[df[col] <= rule].iloc[:, 0]\n",
    "            \n",
    "            # Calculate the number of Good / Bad in each split\n",
    "            goodY = np.count_nonzero(Y == 'good')\n",
    "            badY = np.count_nonzero(Y == 'bad')\n",
    "            goodN = np.count_nonzero(N == 'good')\n",
    "            badN = np.count_nonzero(N == 'bad')\n",
    "            \n",
    "            # Calculate Infomation Gain for the Split\n",
    "            infoGain = InformationGain(goodY,badY,goodN,badN)\n",
    "            \n",
    "            print('---')\n",
    "            print('Splitting on {} > {}'.format(col, rule))\n",
    "            print('# of Good in Y: {}'.format(goodY))\n",
    "            print('# of Bad in Y: {}'.format(badY))\n",
    "            print('# of Good in N: {}'.format(goodN))\n",
    "            print('# of Bad in N: {}'.format(badN))\n",
    "            print(\"Info Gain: {}\".format(infoGain))\n",
    "        \n",
    "    return\n",
    "\n",
    "numercialRules(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Based on your results for parts b and c, what is the optimal binary split of the data?  Of the two child nodes created by this split, which (if any) would require further partitioning? (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: the best binary split is HP > 92.5 because it has the highest info gain of 0.51. For the two child nodes created by this split, HP <= 92.5 requires further partitioning because there is still a mix of Good / Bad in this node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repeat parts a through d until all training data points are perfectly classified by the resulting tree. (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split on HP > 92.5 ---\n",
      "Info Gain: 0.5091859254608121\n",
      "Y Node: 0 Good | 13 Bad\n",
      "N Node: 5 Good | 2 Bad\n",
      "-------------------\n",
      "Next: Split on N node because it is impure\n",
      " \n",
      "--- Split on cylinders = 4 ---\n",
      "Info Gain: 0.8631205685666309\n",
      "Y Node: 5 Good | 0 Bad\n",
      "N Node: 0 Good | 2 Bad\n",
      "-------------------\n",
      "All Child Nodes are Pure. Stop.\n"
     ]
    }
   ],
   "source": [
    "def infoGain(df, col, val, col_type):\n",
    "    '''\n",
    "    This function calculates the information gain given a split condition and counts of labels of the two child nodes\n",
    "    Input: dataframe, column of the split, value of the split, and column type\n",
    "    Output: information gain, list of counts by label\n",
    "    '''\n",
    "    \n",
    "    if col_type == \"cat\":\n",
    "        # Seperate full data based on the split condition\n",
    "        Y = df[df[col] == val].iloc[:, 0]\n",
    "        N = df[df[col] != val].iloc[:, 0]\n",
    "    else: \n",
    "        # Seperate full data based on the split condition\n",
    "        Y = df[df[col] > val].iloc[:, 0]\n",
    "        N = df[df[col] <= val].iloc[:, 0]\n",
    "    \n",
    "    # Calculate the number of Good / Bad in each split\n",
    "    goodY = np.count_nonzero(Y == 'good')\n",
    "    badY = np.count_nonzero(Y == 'bad')\n",
    "    goodN = np.count_nonzero(N == 'good')\n",
    "    badN = np.count_nonzero(N == 'bad')\n",
    "\n",
    "    # Calculate Infomation Gain for the Split\n",
    "    infoGain = InformationGain(goodY,badY,goodN,badN)\n",
    "        \n",
    "    return infoGain, [goodY, badY, goodN, badN]\n",
    "\n",
    "def FindSplit(df):\n",
    "    \n",
    "    '''\n",
    "    This function performs a decision tree process for any given data frame. It recurses until all stop\n",
    "    conditions are met (e.g. no more impurity in child node)\n",
    "    Input: data frame\n",
    "    Output: None\n",
    "    '''\n",
    "    \n",
    "    # Find categorical and numerical columns, exclude target column\n",
    "    cat_col =list(df.iloc[:, 1:len(df.columns)].select_dtypes(include = ['category', 'object']))\n",
    "    num_col =list(df.iloc[:, 1:len(df.columns)].select_dtypes(exclude = ['category', 'object']))\n",
    "    \n",
    "    # Define all splitting rules\n",
    "    cols = []\n",
    "    rules = [] # split condition (e.g. 92.5)\n",
    "    split_name = [] # split name (e.g. HP > 92.5)\n",
    "    info_gain = [] # info gain of this split (e.g. 0.501)\n",
    "    node_mix = [] # count of mixed label at child node (e.g. [0, 13, 0, 5])\n",
    "    \n",
    "    # Calculate info gain, child node impurity, and log split details for Categorical Columns\n",
    "    for col in cat_col:\n",
    "        unique_val = np.unique(df[col])\n",
    "        for val in unique_val:\n",
    "            cols.append([col, \"cat\"])\n",
    "            rules.append(val)\n",
    "            split_name.append(str(col) + \" = \" + str(val))\n",
    "            \n",
    "            gain, mix = infoGain(df, col, val, 'cat')\n",
    "            info_gain.append(gain)\n",
    "            node_mix.append(mix)\n",
    "    \n",
    "    # Calculate info gain, child node impurity, and log split details for Numerical Columns\n",
    "    for col in num_col:\n",
    "        unique_val = np.unique(df[col])\n",
    "        for i in range(0, len(unique_val)):\n",
    "            try:\n",
    "                midpoint = (unique_val[i]+unique_val[i+1])/2\n",
    "                cols.append([col, \"num\"])\n",
    "                rules.append(midpoint)\n",
    "                split_name.append(str(col) + \" > \" + str(midpoint))\n",
    "                \n",
    "                gain, mix = infoGain(df, col, midpoint, 'num')\n",
    "                info_gain.append(gain)\n",
    "                node_mix.append(mix)\n",
    "            \n",
    "            except IndexError:\n",
    "                pass\n",
    "    \n",
    "    # Get the Index of the split condition based on Info Gain\n",
    "    best_split_idx = np.argmax(info_gain)\n",
    "    \n",
    "    print('--- Split on {} ---'.format(split_name[best_split_idx]))\n",
    "    print('Info Gain: {}'.format(info_gain[best_split_idx]))\n",
    "    print('Y Node: {} Good | {} Bad'.format(node_mix[best_split_idx][0], node_mix[best_split_idx][1]))\n",
    "    print('N Node: {} Good | {} Bad'.format(node_mix[best_split_idx][2], node_mix[best_split_idx][3]))\n",
    "    print('-------------------')\n",
    "    \n",
    "    # Recurse FindSplit() on child node(s)\n",
    "    # Child Node Impurity Count in this list format [goodY, badY, goodN, badN]\n",
    "    \n",
    "    if node_mix[best_split_idx][0] != 0 and node_mix[best_split_idx][1] !=0:\n",
    "        \n",
    "        # recurse splitting on the Y node\n",
    "        print('Next: Split on Y node because it is impure')\n",
    "        print(' ')\n",
    "        \n",
    "        # Subset Child Node based on Categorical / Numerical Condition\n",
    "        if cols[best_split_idx][1] == \"cat\":\n",
    "            FindSplit(df[df[cols[best_split_idx][0]] == rules[best_split_idx]])\n",
    "        else: \n",
    "            FindSplit(df[df[cols[best_split_idx][0]] > rules[best_split_idx]])\n",
    "    \n",
    "    elif node_mix[best_split_idx][2] != 0 and node_mix[best_split_idx][3] !=0:\n",
    "       \n",
    "        # recurse splitting on the N node\n",
    "        print('Next: Split on N node because it is impure')\n",
    "        print(' ')\n",
    "        \n",
    "        # Subset Child Node based on Categorical / Numerical Condition\n",
    "        if cols[best_split_idx][1] == \"cat\":\n",
    "            FindSplit(df[df[cols[best_split_idx][0]] != rules[best_split_idx]])\n",
    "        else: \n",
    "            FindSplit(df[df[cols[best_split_idx][0]] <= rules[best_split_idx]])\n",
    "    else:\n",
    "        print('All Child Nodes are Pure. Stop.')\n",
    "        \n",
    "    return\n",
    "\n",
    "FindSplit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Draw or show the final decision tree in a format of your choice.  The decision to make at each step and the predicted value at each leaf node must be clear. (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: See output from cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Classify each of the following four vehicles as having \"good\" or \"bad\" fuel efficiency (miles per gallon).  Do this by hand using the tree structure learned in part f. (4 pts)\n",
    "\n",
    "?,8,70,light -> Bad\n",
    "\n",
    "?,6,113,medium -> Bad\n",
    "\n",
    "?,4,83,weighty -> Good\n",
    "\n",
    "?,4,95,weighty -> Bad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3, Predicting burden of disease （40 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>FrxnPeaceIn10</th>\n",
       "      <th>ODA4H2OPcptaDol</th>\n",
       "      <th>RenewResm3PcptaYr</th>\n",
       "      <th>SustAccImprWatRur</th>\n",
       "      <th>SustAccImprWatUrb</th>\n",
       "      <th>SustAccImprSanRur</th>\n",
       "      <th>SustAccImprSanUrb</th>\n",
       "      <th>TotHlthExpPctofGDP</th>\n",
       "      <th>GenGovtPctofTotHlthExp</th>\n",
       "      <th>ExtResHlthPctTotExpHlth</th>\n",
       "      <th>PCptaGovtExpHlthAvgExcRt</th>\n",
       "      <th>GDPPCptaIntDol</th>\n",
       "      <th>AdultLtrcyRate</th>\n",
       "      <th>FemaleLtrcyRate</th>\n",
       "      <th>BurdenOfDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2986</td>\n",
       "      <td>0.10891</td>\n",
       "      <td>0.18812</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.15842</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.4560</td>\n",
       "      <td>4</td>\n",
       "      <td>430</td>\n",
       "      <td>0.35644</td>\n",
       "      <td>0.20792</td>\n",
       "      <td>awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.58</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.94059</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.801980</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>49</td>\n",
       "      <td>6158</td>\n",
       "      <td>0.85644</td>\n",
       "      <td>0.78713</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>473</td>\n",
       "      <td>0.79208</td>\n",
       "      <td>0.91089</td>\n",
       "      <td>0.811880</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>71</td>\n",
       "      <td>4860</td>\n",
       "      <td>0.69307</td>\n",
       "      <td>0.60396</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  FrxnPeaceIn10  ODA4H2OPcptaDol  RenewResm3PcptaYr  \\\n",
       "0  Afghanistan            0.1             0.16               2986   \n",
       "1      Albania            1.0             5.58              13306   \n",
       "2      Algeria            0.0             0.33                473   \n",
       "\n",
       "   SustAccImprWatRur  SustAccImprWatUrb  SustAccImprSanRur  SustAccImprSanUrb  \\\n",
       "0            0.10891            0.18812           0.049505            0.15842   \n",
       "1            0.94059            0.98020           0.801980            0.98020   \n",
       "2            0.79208            0.91089           0.811880            0.98020   \n",
       "\n",
       "   TotHlthExpPctofGDP  GenGovtPctofTotHlthExp  ExtResHlthPctTotExpHlth  \\\n",
       "0               0.065                   0.395                   0.4560   \n",
       "1               0.065                   0.417                   0.0340   \n",
       "2               0.041                   0.808                   0.0005   \n",
       "\n",
       "   PCptaGovtExpHlthAvgExcRt  GDPPCptaIntDol  AdultLtrcyRate  FemaleLtrcyRate  \\\n",
       "0                         4             430         0.35644          0.20792   \n",
       "1                        49            6158         0.85644          0.78713   \n",
       "2                        71            4860         0.69307          0.60396   \n",
       "\n",
       "  BurdenOfDisease  \n",
       "0           awful  \n",
       "1             low  \n",
       "2            high  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Burden of diarrheal illness by country.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary\n",
    "\n",
    "NAME: Burden of diarrheal illness by country\n",
    "\n",
    "SIZE: 130 Countries, 16 Variables\n",
    "\n",
    "VARIABLE DESCRIPTIONS:\n",
    "\n",
    "Country: Country name\n",
    "\n",
    "FrxnPeaceIn10: Fraction of the past ten years in which a country has been at peace \n",
    "\n",
    "ODA4H2OPcptaDol: Per Capita Official Developmental Assistance for water projects\n",
    "\n",
    "RenewResm3PcptaYr: Renewable Water Resources in cubic meters per capita per year\n",
    "\n",
    "SustAccImprWatRur: Fraction of rural population with sustainable access to improved water\n",
    "\n",
    "SustAccImprWatUrb: Fraction of urban population with sustainable access to improved water\n",
    "\n",
    "SustAccImprSanRur: Fraction of rural population with sustainable access to improved sanitation\n",
    "\n",
    "SustAccImprSanUrb: Fraction of urban population with sustainable access to improved sanitation\n",
    "\n",
    "TotHlthExpPctofGDP: Fraction of a country's GDP devoted to health spending\n",
    "\n",
    "GenGovtPctofTotHlthExp: The fraction of total health expenditures for a country which is provided by the government\n",
    "\n",
    "ExtResHlthPctTotExpHlth: The fraction of total health expenditures for a country which is comes from sources external to the country\n",
    "\n",
    "PCptaGovtExpHlthAvgExcRt: Per Capita Government Health Expenditures at the average exchange rate\n",
    "\n",
    "GDPPCptaIntDol: Gross Domestic Product per capita in international dollars\n",
    "\n",
    "AdultLtrcyRate: Adult Literacy rate\n",
    "\n",
    "FemaleLtrcyRate: Female Literacy rate\n",
    "\n",
    "BurdenOfDisease: Our target variable for classification.  The burden of disease due to diarrheal illness, categorized into \"low\", \"medium\", \"high\", and \"awful\" quartiles.  For each country, we have estimates of the number of Disability-Adjusted Life Years lost per 1000 persons per year (DALYs) due to diarrheal illness.  Countries with \"low\" burden of disease have up to 2.75345 DALYs; countries with \"medium\" burden of disease have between 2.75345 and 8.2127 DALYs; countries with \"high\" burden of disease have between 8.2127 and 26.699 DALYs; and countries with \"awful\" burden of diease have more than 26.699 DALYs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your goal is to train a decision tree classifier for the attribute “BurdenOfDisease\" using all other variables (except country name) as features with sklearn.tree.DecisionTreeClassifier. \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Please choose a train/test split and choose a hyper-parameter governing model simplicity, for example, the maximum tree depth or maximum number of leaf nodes. Then, fit your decision tree classifier (using the training set) for different values of this parameter and for each such value, record the corresponding classification accuracy on the test set. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Tree Depth ---\n",
      "[ 2  4  6  8 10 12]\n",
      "---  Accuracy ---\n",
      "0.604651162791\n",
      "0.697674418605\n",
      "0.651162790698\n",
      "0.651162790698\n",
      "0.651162790698\n",
      "0.651162790698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "seed = 123\n",
    "    \n",
    "def encoding(df):\n",
    "    \n",
    "    '''\n",
    "    This function performs label encoding for categorical columns with text values\n",
    "    Input: data frame\n",
    "    Output: data frame with encoded values\n",
    "    '''\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    columnsToEncode = list(df.select_dtypes(include = ['category', 'object']))\n",
    "    le = LabelEncoder()\n",
    "    for feature in columnsToEncode:\n",
    "        try:\n",
    "            df[feature] = le.fit_transform(df[feature])\n",
    "        except:\n",
    "            print(\"Error Encoding: {}\".format(feature))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run a Decision Tree\n",
    "def treeDepthAnalysis(X_train, X_test, Y_train, Y_test, min_depth, max_depth):\n",
    "    \n",
    "    '''\n",
    "    This function performs analysis of accuracy over a range of decision tree depth range. Tree depth increments \n",
    "    by two.\n",
    "    Inputs: \n",
    "        - X_train: independent var of training set\n",
    "        - X_test: independent var of testing set\n",
    "        - Y_train: dependent var of training set\n",
    "        - Y_test: dependent var of testing set\n",
    "        - min_depth: the minimum tree depth of the analysis\n",
    "        - max_depth: the maximum tree depth of the analysis\n",
    "    Outputs:\n",
    "        - depth_range: a list of tree depth used in the analysis\n",
    "        - results: a list of accuracy correspond to the tree depth\n",
    "    '''\n",
    "    \n",
    "    depth_range = np.arange(min_depth, max_depth + 2, 2)\n",
    "    print('---  Tree Depth ---')\n",
    "    print(depth_range)\n",
    "    print('---  Accuracy ---')\n",
    "    \n",
    "    results = []\n",
    "    for depth in depth_range: \n",
    "        clf = DecisionTreeClassifier(max_depth = depth, random_state = seed)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        Y_predict = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, Y_predict)\n",
    "        results.append(accuracy)\n",
    "        print(accuracy)\n",
    "    \n",
    "    return depth_range, results\n",
    "\n",
    "\n",
    "### MAIN Execution Process ###\n",
    "\n",
    "# Encode Data\n",
    "encoded_data = encoding(data)\n",
    "\n",
    "# Split Data to Training and Testing\n",
    "X = encoded_data.iloc[:, 0:len(encoded_data.columns)-1]\n",
    "Y = encoded_data['BurdenOfDisease']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = seed)\n",
    "\n",
    "# Run the Tree Depth Analysis\n",
    "depth_range, results = treeDepthAnalysis(X_train, X_test, Y_train, Y_test, 2, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Make a plot of accuracy vs. simplicity for different values of the hyper-parameter chosen in part a). That is, the x-axis should be hyper-parameter value (e.g. tree depth) and the y-axis should be accuracy. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1931f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(depth_range, results);\n",
    "plt.xlabel('Tree Depth');\n",
    "plt.ylabel('Accuracy on Test Dataset');\n",
    "plt.title('Tree Depth vs. Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Tune the hyper-parameter you choose in part a) by cross-validation using the training data. You can choose to use the GridSearchCV package from sklearn or write your own code to do cross-validation by spliting the training data into training and validation data. What is the out of sample accuracy after tuning the hyper-parameter? (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Grid Search Results ---\n",
      "[mean: 0.62069, std: 0.01921, params: {'max_depth': 2},\n",
      " mean: 0.62069, std: 0.04128, params: {'max_depth': 4},\n",
      " mean: 0.64368, std: 0.04194, params: {'max_depth': 6},\n",
      " mean: 0.64368, std: 0.04194, params: {'max_depth': 8},\n",
      " mean: 0.64368, std: 0.04194, params: {'max_depth': 10},\n",
      " mean: 0.64368, std: 0.04194, params: {'max_depth': 12}]\n",
      "--- Tree Depth with the Best CV Accuracy ---\n",
      "Max Tree Depth: 6\n",
      "--- Result on Hold-Out Sampel using Best Tree Depth ---\n",
      "0.651162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ian/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "def GridSearch(X_train, Y_train, model, params, params_values, nfold):\n",
    "    '''\n",
    "    This function performs a Grid Search to find the Maximum Tree Depth that provides\n",
    "    the best accuracy based on Cross Validation.\n",
    "    Inputs:\n",
    "        - X_train: independent vars of training set\n",
    "        - Y_train: dependent vars of training set\n",
    "        - model: model object for grid search\n",
    "        - params: a list of model parameters for grid search (e.g. [max_depth])\n",
    "        - params_values: a list of parameter values for grid search (e.g. [[2, 6, 8, 10]])\n",
    "        - nfold: number of folds for cross validation\n",
    "    Outputs: the Best Tree Depth\n",
    "    '''\n",
    "    for i in range(0, len(params)):\n",
    "        param_grid = {params[i]:params_values[i]}\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid, cv=nfold)\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    best = grid_search.best_params_['max_depth']\n",
    "    \n",
    "    print('--- Grid Search Results ---')\n",
    "    pprint(grid_search.grid_scores_)\n",
    "    print('--- Tree Depth with the Best CV Accuracy ---')\n",
    "    print('Max Tree Depth: {}'.format(best))\n",
    "    \n",
    "    return best\n",
    "\n",
    "def TunedTree(X_train, X_test, Y_train, Y_test, best_depth):\n",
    "    '''\n",
    "    This function learn a decision tree based on the best max tree depth; then test the accuracy on a hold-out sample.\n",
    "    Inputs: \n",
    "        - X_train: independent var of training set\n",
    "        - X_test: independent var of testing set\n",
    "        - Y_train: dependent var of training set\n",
    "        - Y_test: dependent var of testing set\n",
    "        - best_depth: the best Max Tree Depth based on Grid Search\n",
    "    Outputs: None\n",
    "    '''\n",
    "    \n",
    "    clf = clf = DecisionTreeClassifier(random_state = 123, max_depth = best_depth)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predict = clf.predict(X_test)\n",
    "    print('--- Result on Hold-Out Sampel using Best Tree Depth ---')\n",
    "    print(accuracy_score(Y_test, Y_predict))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "### MAIN Execution Process ###\n",
    "\n",
    "# Find the Best Tree Depth using Grid Search and Cross Validation\n",
    "best_depth = GridSearch(X_train, Y_train, \n",
    "                        DecisionTreeClassifier(random_state = 123), \n",
    "                        ['max_depth'], [[2, 4, 6, 8, 10, 12]], 3)\n",
    "\n",
    "# Measure Accuracy on Hold-out Samples using the Best Tree Depth\n",
    "TunedTree(X_train, X_test, Y_train, Y_test, best_depth)\n",
    "\n",
    "# Reference: https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Visualize a simple decision tree (e.g., with max_depth = 2 or 3) learned from the data.  To do so, given your decision tree dt, you can use the code below, then copy and paste the resulting output into http://www.webgraphviz.com.  Alternatively, if you have graphviz installed on your machine, you can use that. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types dtype('<U75') dtype('<U75') dtype('<U75')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-58f397b470c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                          \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                          \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                          special_characters=False,impurity=False)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"impurity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mrecurse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# If required, draw leaf nodes at same depth as each other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mrecurse\u001b[0;34m(tree, node_id, criterion, parent, depth)\u001b[0m\n\u001b[1;32m    330\u001b[0m             out_file.write('%d [label=%s'\n\u001b[1;32m    331\u001b[0m                            % (node_id,\n\u001b[0;32m--> 332\u001b[0;31m                               node_to_str(tree, node_id, criterion)))\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/export.py\u001b[0m in \u001b[0;36mnode_to_str\u001b[0;34m(tree, node_id, criterion)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                           \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                                           characters[2])\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mnode_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# Clean up any trailing newlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types dtype('<U75') dtype('<U75') dtype('<U75')"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=123, max_depth=best_depth)\n",
    "dt.fit(X_train,Y_train)\n",
    "\n",
    "thestring=tree.export_graphviz(dt,out_file=None,\n",
    "                         feature_names=X_train.columns.values,  \n",
    "                         class_names=dt.classes_,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False)\n",
    "print(thestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4, Fit a random forest to the data from question 3 (20 pts)\n",
    "\n",
    "a) Please use the same test/train split from previous question and feel free to tune the hyper-parameters for Random Forest model using training data. The package from sklearn is here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.\n",
    "Then please report your out of sample prediction result and compare this model's performance with 3c). (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Write one paragraph comparing the results from those two models (Random Forest vs Decision Tree) in terms of both accuracy and interpretability. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Your answer here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
